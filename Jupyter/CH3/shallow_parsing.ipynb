{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/distillery/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "The brown fox is quick and he is jumping over the lazy dog\n",
      "[Chunk('The brown fox/NP'), Chunk('is/VP'), Chunk('quick/ADJP'), Chunk('he/NP'), Chunk('is jumping/VP'), Chunk('over/PP'), Chunk('the lazy dog/NP')]\n",
      "NP -> [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]\n",
      "VP -> [('is', 'VBZ')]\n",
      "ADJP -> [('quick', 'JJ')]\n",
      "NP -> [('he', 'PRP')]\n",
      "VP -> [('is', 'VBZ'), ('jumping', 'VBG')]\n",
      "PP -> [('over', 'IN')]\n",
      "NP -> [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk; nltk.download('sentiwordnet')\n",
    "from pattern.en import parsetree, Chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "tree = parsetree(sentence)\n",
    "print(tree)\n",
    "\n",
    "for sentence_tree in tree:\n",
    "    print(sentence_tree.chunks)\n",
    "    \n",
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print(chunk.type, '->', [(word.string, word.type) \n",
    "                                 for word in chunk.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n",
      "[('NP', [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]), ('VP', [('is', 'VBZ')]), ('ADJP', [('quick', 'JJ')]), ('-', [('and', 'CC')]), ('NP', [('he', 'PRP')]), ('VP', [('is', 'VBZ'), ('jumping', 'VBG')]), ('PP', [('over', 'IN')]), ('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])]\n",
      "(S\n",
      "  (NP (DT The) (JJ brown) (NN fox))\n",
      "  (VP (VBZ is))\n",
      "  (ADJP (JJ quick))\n",
      "  (- (CC and))\n",
      "  (NP (PRP he))\n",
      "  (VP (VBZ is) (VBG jumping))\n",
      "  (PP (IN over))\n",
      "  (NP (DT the) (JJ lazy) (NN dog)))\n"
     ]
    }
   ],
   "source": [
    "def create_sentence_tree(sentence, lemmatize=False):\n",
    "    sentence_tree = parsetree(sentence, \n",
    "                              relations=True, \n",
    "                              lemmata=lemmatize)\n",
    "    return sentence_tree[0]\n",
    "    \n",
    "def get_sentence_tree_constituents(sentence_tree):\n",
    "    return sentence_tree.constituents()\n",
    "    \n",
    "def process_sentence_tree(sentence_tree):\n",
    "    \n",
    "    tree_constituents = get_sentence_tree_constituents(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        (item.type,\n",
    "                         [\n",
    "                             (w.string, w.type)\n",
    "                             for w in item.words\n",
    "                         ]\n",
    "                        )\n",
    "                        if type(item) == Chunk\n",
    "                        else ('-',\n",
    "                              [\n",
    "                                   (item.string, item.type)\n",
    "                              ]\n",
    "                             )\n",
    "                             for item in tree_constituents\n",
    "                    ]\n",
    "    \n",
    "    return processed_tree\n",
    "    \n",
    "def print_sentence_tree(sentence_tree):\n",
    "    \n",
    "\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        Tree( item[0],\n",
    "                             [\n",
    "                                 Tree(x[1], [x[0]])\n",
    "                                 for x in item[1]\n",
    "                             ]\n",
    "                            )\n",
    "                            for item in processed_tree\n",
    "                     ]\n",
    "\n",
    "    tree = Tree('S', processed_tree )\n",
    "    print(tree)\n",
    "    \n",
    "def visualize_sentence_tree(sentence_tree):\n",
    "    \n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        Tree( item[0],\n",
    "                             [\n",
    "                                 Tree(x[1], [x[0]])\n",
    "                                 for x in item[1]\n",
    "                             ]\n",
    "                            )\n",
    "                            for item in processed_tree\n",
    "                     ]\n",
    "    tree = Tree('S', processed_tree )\n",
    "    tree.draw()\n",
    "    \n",
    "    \n",
    "t = create_sentence_tree(sentence)\n",
    "print(t)\n",
    "\n",
    "pt = process_sentence_tree(t)\n",
    "print(pt)\n",
    "\n",
    "print_sentence_tree(t)\n",
    "visualize_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree](img/shallow_parsing1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/distillery/MyProjects/nlp/lib/python3.5/site-packages/nltk/tokenize/regexp.py:125: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('quick', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]\n",
    "\n",
    "simple_sentence = 'the quick fox jumped over the lazy dog'\n",
    "\n",
    "from nltk.chunk import RegexpParser\n",
    "from pattern.en import tag\n",
    "\n",
    "tagged_simple_sent = tag(simple_sentence)\n",
    "print(tagged_simple_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT quick/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT quick/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "chink_grammar = \"\"\"\n",
    "NP: {<.*>+} # chunk everything as NP\n",
    "}<VBD|IN>+{\n",
    "\"\"\"\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP is/VBZ jumping/VBG)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  54.5%%\n",
      "    Precision:     25.0%%\n",
      "    Recall:        52.5%%\n",
      "    F-Measure:     33.9%%\n"
     ]
    }
   ],
   "source": [
    "tagged_sentence = tag(sentence)\n",
    "print(tagged_sentence)\n",
    "\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\n",
    "\"\"\"\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_sentence)\n",
    "\n",
    "print(c)\n",
    "\n",
    "print(rc.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)\n",
    "\n",
    "wtc = tree2conlltags(train_sent)\n",
    "wtc\n",
    "\n",
    "tree = conlltags2tree(wtc)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.6%%\n",
      "    Precision:     98.4%%\n",
      "    Recall:       100.0%%\n",
      "    F-Measure:     99.2%%\n",
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  is/VBZ\n",
      "  (NP quick/JJ)\n",
      "  and/CC\n",
      "  (NP he/PRP)\n",
      "  is/VBZ\n",
      "  jumping/VBG\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "  tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "  \n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "  def __init__(self, train_sentences, \n",
    "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "  def parse(self, tagged_sentence):\n",
    "    if not tagged_sentence: \n",
    "        return None\n",
    "    pos_tags = [tag for word, tag in tagged_sentence]\n",
    "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                     in zip(tagged_sentence, chunk_tags)]\n",
    "    return conlltags2tree(wpc_tags)\n",
    "\n",
    "\n",
    "ntc = NGramTagChunker(train_data)\n",
    "print(ntc.evaluate(test_data))\n",
    "\n",
    "tree = ntc.parse(tagged_sentence)\n",
    "print(tree)\n",
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree2](img/shallow_parsing2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /home/distillery/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.4%%\n",
      "    Precision:     80.8%%\n",
      "    Recall:        86.0%%\n",
      "    F-Measure:     83.3%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "nltk.download('conll2000')\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:7500]\n",
    "test_wsj_data = wsj_data[7500:]\n",
    "print(train_wsj_data[10])\n",
    "\n",
    "tc = NGramTagChunker(train_wsj_data)\n",
    "print(tc.evaluate(test_wsj_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
