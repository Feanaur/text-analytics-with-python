{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Sample document: the blood of the lamb.\n",
      "\n",
      "This will be a hard task, because most cultures used most animals\n",
      "for blood sacrifices. It has to be something related to our current\n",
      "post-modernism state. Hmm, what about used computers?\n",
      "\n",
      "Cheers,\n",
      "Kent\n",
      "Class label: 19\n",
      "Actual class label: talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(\n",
    "        subset='all',\n",
    "        shuffle=True,\n",
    "        remove=('headers', 'footers', 'quotes')\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "        corpus,\n",
    "        labels,\n",
    "        test_size=0.33,\n",
    "        random_state=42\n",
    "    )\n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels\n",
    "\n",
    "\n",
    "dataset = get_data()\n",
    "\n",
    "print(dataset.target_names)\n",
    "\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "\n",
    "print('Sample document:', corpus[10])\n",
    "print('Class label:', labels[10])\n",
    "print('Actual class label:', dataset.target_names[labels[10]])\n",
    "\n",
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,\n",
    "                                                                        labels,\n",
    "                                                                        test_data_proportion=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization import normalize_corpus\n",
    "\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extractors import bow_extractor, tfidf_extractor\n",
    "from feature_extractors import averaged_word_vectorizer\n",
    "from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# bag of words features\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "# tfidf features\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text) for text in norm_train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text) for text in norm_test_corpus]\n",
    "# build word2vec model\n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)\n",
    "\n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)\n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)\n",
    "\n",
    "\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                                  tfidf_vectors=tfidf_train_features,\n",
    "                                                                  tfidf_vocabulary=vocab,\n",
    "                                                                  model=model,\n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                                 tfidf_vectors=tfidf_test_features,\n",
    "                                                                 tfidf_vocabulary=vocab,\n",
    "                                                                 model=model,\n",
    "                                                                 num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "\n",
    "    print(\n",
    "        'Accuracy:',\n",
    "        np.round(\n",
    "            metrics.accuracy_score(\n",
    "                true_labels,\n",
    "                predicted_labels\n",
    "            ), 2)\n",
    "    )\n",
    "    print('Precision:', np.round(\n",
    "        metrics.precision_score(\n",
    "            true_labels,\n",
    "            predicted_labels,\n",
    "            average='weighted'\n",
    "        ), 2)\n",
    "    )\n",
    "    print('Recall:', np.round(\n",
    "        metrics.recall_score(\n",
    "            true_labels,\n",
    "            predicted_labels,\n",
    "            average='weighted'),\n",
    "        2)\n",
    "    )\n",
    "    print('F1 Score:', np.round(\n",
    "        metrics.f1_score(\n",
    "            true_labels,\n",
    "            predicted_labels,\n",
    "            average='weighted'),\n",
    "        2)\n",
    "    )\n",
    "\n",
    "\n",
    "def train_predict_evaluate_model(classifier,\n",
    "                                 train_features, train_labels,\n",
    "                                 test_features, test_labels):\n",
    "    # build model\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    # evaluate model prediction performance\n",
    "    get_metrics(true_labels=test_labels,\n",
    "                predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n",
      "Precision: 0.72\n",
      "Recall: 0.67\n",
      "F1 Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/distillery/MyProjects/nlp/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "Precision: 0.65\n",
      "Recall: 0.62\n",
      "F1 Score: 0.62\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Precision: 0.78\n",
      "Recall: 0.72\n",
      "F1 Score: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes with tfidf features\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/distillery/MyProjects/nlp/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.77\n",
      "Recall: 0.77\n",
      "F1 Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf features\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/distillery/MyProjects/nlp/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "Precision: 0.59\n",
      "Recall: 0.54\n",
      "F1 Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with averaged word vector features\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/distillery/MyProjects/nlp/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n",
      "Precision: 0.55\n",
      "Recall: 0.53\n",
      "F1 Score: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism -> soc.religion.christian\n",
      "talk.politics.misc -> talk.politics.guns\n",
      "talk.religion.misc -> soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=range(0,20), columns=range(0,20))\n",
    "\n",
    "class_names = dataset.target_names\n",
    "print(class_names[0], '->', class_names[15])\n",
    "print(class_names[18], '->', class_names[16])\n",
    "print(class_names[19], '->', class_names[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "I would like a list of Bible contadictions from those of you who dispite being free from Christianity are well versed in the Bible. \n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "  They spent quite a bit of time on the wording of the Constitution.  They picked words whose meanings implied the intent.  We have already looked in the dictionary to define the word.  Isn't this sufficient?   But we were discussing it in relation to the death penalty.  And, the Constitution need not define each of the words within.  Anyone who doesn't know what cruel is can look in the dictionary (and we did).\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "Our Lord and Savior David Keresh has risen!   \tHe has been seen alive!   \tSpread the word!     --------------------------------------------------------------------------------\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "  \"This is your god\" (from John Carpenter's \"They Live,\" natch)  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 15:\n",
    "        print('Actual Label:', class_names[label])\n",
    "        print('Predicted Label:', class_names[predicted_label])\n",
    "        print('Document:-')\n",
    "        print(re.sub('\\n', ' ', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "After the initial gun battle was over, they had 50 days to come out peacefully. They had their high priced lawyer, and judging by the posts here they had some public support. Can anyone come up with a rational explanation why the didn't come out (even after they negotiated coming out after the radio sermon) that doesn't include the Davidians wanting to commit suicide/murder/general mayhem?\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "Yesterday, the FBI was saying that at least three of the bodies had gunshot wounds, indicating that they were shot trying to escape the fire.  Today's paper quotes the medical examiner as saying that there is no evidence of gunshot wounds in any of the recovered bodies.  At the beginning of this siege, it was reported that while Koresh had a class III (machine gun) license, today's paper quotes the government as saying, no, they didn't have a license.  Today's paper reports that a number of the bodies were found with shoulder weapons next to them, as if they had been using them while dying -- which doesn't sound like the sort of action I would expect from a suicide.  Our government lies, as it tries to cover over its incompetence and negligence.  Why should I believe the FBI's claims about anything else, when we can see that they are LYING?  This system of government is beyond reform.\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "  Well, for one thing most, if not all the Dividians (depending on whether they could show they acted in self-defense and there were no illegal weapons), could have gone on with their life as they were living it. No one was forcing them to give up their religion or even their legal weapons. The Dividians had survived a change in leadership before so even if Koresch himself would have been convicted and sent to jail, they still could have carried on.   I don't think the Dividians were insane, but I don't see a reason for mass suicide (if the fire was intentional set by some of the Dividians.) We also don't know that, if the fire was intentionally set from inside, was it a generally know plan or was this something only an inner circle knew about, or was it something two or three felt they had to do with or without Koresch's knowledge/blessing, etc.? I don't know much about Masada. Were some people throwing others over? Did mothers jump over with their babies in their arms?\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "rja@mahogany126.cray.com (Russ Anderson) writes...     \tThe fact is that Koresh and his followers involved themselves \tin a gun battle to control the Mt Carmel complex. That is not \tin dispute. From what I remember of the trial, the authories \tcouldn't reasonably establish who fired first, the big reason \tbehind the aquittal.                    _____  _____                   \\\\\\\\\\\\/ ___/___________________   Mitchell S Todd  \\\\\\\\/ /                 _____/__________________________ ________________    \\\\/ / mst4298@zeus._____/.'.'.'.'.'.'.'.'.'.'.'.'_'_'_/ \\_____        \\__    / / tamu.edu  _____/.'.'.'.'.'.'.'.'.'.'.'.'.'_'_/     \\__________\\__  / /        _____/_'_'_'_'_'_'_'_'_'_'_'_'_'_'_/                 \\_ / /__________/                  \\/____/\\\\\\\\\\\\  \t\t\t \\\\\\\\\\\\\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 18 and predicted_label == 16:\n",
    "        print('Actual Label:', class_names[label])\n",
    "        print('Predicted Label:', class_names[predicted_label])\n",
    "        print('Document:-')\n",
    "        print(re.sub('\\n', ' ', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
